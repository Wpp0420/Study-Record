# 0 各优化算法的联邦学习

机器学习问题通常会转换成一个目标函数去求解,优化算法是求解目标函数中参数的重要工具.在大数据环境下,需要设计并行与分布式的优化算法,通过多核计算和分布式计算技术来加速训练过程.近年来,该领域涌现了大量研究工作,部分算法也在各机器学习平台得到广泛应用,本课题围绕**梯度下降算法、二阶优化算法、邻近梯度算法、坐标下降算法、交替方向乘子算法**这5类最常见的优化方法展开研究,每一类算法分别从单机并行和分布式并行来分析相关研究成果,并从模型特性、输入数据特性、算法评价、并行计算模型等角度对每种算法进行详细对比. 针对现有算法存在缺陷，加以改进，设计更加高效的并行分布式优化算法。

# 1.梯度下降法 gradient descent

McMahan, Brendan, et al. "Communication-efficient learning of deep networks from decentralized data." *Artificial intelligence and statistics*. PMLR, 2017.

![image-20240423213055587](Piuture_0%20%E5%90%84%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%E7%9A%84%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0/image-20240423213055587.png)



# 2.二阶优化算法 second-order optimization algorithm

![image-20240423213332361](Piuture_0%20%E5%90%84%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%E7%9A%84%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0/image-20240423213332361.png)

文章：

Over-the-Air Federated Learning via Second-Order Optimization

P. Yang, Y. Jiang, T. Wang, Y. Zhou, Y. Shi and C. N. Jones, "Over-the-Air Federated Learning via Second-Order Optimization," in IEEE Transactions on Wireless Communications, vol. 21, no. 12, pp. 10560-10575, Dec. 2022, doi: 10.1109/TWC.2022.3185156.
keywords: {Optimization;Convergence;Servers;Federated learning;Performance evaluation;Collaborative work;Behavioral sciences;Federated learning;over-the-air computation;second-order optimization method},

代码：https://github.com/Golden-Slumber/AirFL-2nd

# 3.邻近梯度算法 proximal gradient algorithms

![image-20240423213318870](Piuture_0%20%E5%90%84%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%E7%9A%84%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0/image-20240423213318870.png)

文章：
Federated Learning with Proximal Stochastic Variance Reduced Gradient Algorithms

Rostami, Mohammad Bagher and Solmaz S. Kia. “Federated Learning Using Variance Reduced Stochastic Gradient for Probabilistically Activated Agents.” *2023 American Control Conference (ACC)* (2022): 861-866.

代码：

https://github.com/CharlieDinh/FederatedLearningWithSVRG

# 4.坐标下降法 coordinate descent

![image-20240423214603883](Piuture_0%20%E5%90%84%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%E7%9A%84%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0/image-20240423214603883.png)

文章

Federated Block Coordinate Descent Scheme for Learning Global and Personalized Models

Wu, Ruiyuan, et al. "Federated block coordinate descent scheme for learning global and personalized models." *Proceedings of the AAAI Conference on Artificial Intelligence*. Vol. 35. No. 12. 2021.

代码：

https://github.com/REIYANG/FedBCD

# 5.交替方向算子法 Alternating Direction Method of Multipliers

![image-20240423214612206](Piuture_0%20%E5%90%84%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%E7%9A%84%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0/image-20240423214612206.png)

文章：

Federated Learning via Inexact ADMM

Zhou, Shenglong, and Geoffrey Ye Li. "Federated learning via inexact ADMM." *IEEE Transactions on Pattern Analysis and Machine Intelligence* (2023).

代码：

https://github.com/ShenglongZhou/FedADMM

