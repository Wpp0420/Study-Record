# 5 差分隐私

# 0 正文



# 1 Ref1基于差分隐私的联邦学习数据隐私安全技术

https://mp.weixin.qq.com/s/W5lt9x48Y1HfOoI7HyRHwg

==相邻数据集==。现给定两个数据集D和D’, 若它们有且仅有一条数据不一样，那我们就称此二者为相邻数据集

==随机化算法== A （所谓随机化算法，是指对于特定输入，该算法的输出不是固定值，而是服从某一分布），其分别作用于两个相邻数据集得到的两个输出分布难以区分。

==差分隐私==，顾名思义就是用来防范差分攻击的，举个简单的例子，假设现在有一个婚恋数据库，2个单身8个已婚，只能查有多少人单身。刚开始的时候查询发现，2个人单身；现在张三跑去登记了自己婚姻状况，再一查，发现3个人单身。所以张三单身。这里张三作为一个样本的的出现，使得攻击者获得了奇怪的知识。而差分隐私需要做到的就是使得攻击者的知识不会因为这些新样本的出现而发生变化。
通过严格的数学证明，使用随机应答（Randomized Response）方法确保数据集在输出信息时受单条记录的影响始终低于某个阈值，从而使第三方无法根据输出的变化判断单条记录的更改或增删，被认为是目前基于扰动的隐私保护方法中安全级别最高的方法。

==差分隐私效果==

**其分别作用于两个相邻数据集得到的两个输出分布难以区分**

![img](Piuture_5%20%E5%B7%AE%E5%88%86%E9%9A%90%E7%A7%81/cd27fc1cd0a2455f874e233001057037.png)

也就是说，如果该算法作用于任何相邻数据集，得到一个特定输出 *O* 的概率应差不多，那么我们就说这个算法能达到差分隐私的效果。也就是说，**观察者通过==观察输出结果==很难察觉出数据集一点微小的变化**，从而达到保护隐私的目的

==如何才能得到差分隐私==

**最简单的方法是加噪音**，也就是在输入或输出上加入随机化的噪音，以期将真实数据掩盖掉。比较常用的是加拉普拉斯噪音。由于拉普拉斯分布的数学性质正好与差分隐私的定义相契合，因此很多研究和应用都采用了此种噪音。

以前面那个数据集为例，假设我们想要知道到底有多少人是单身狗，我们只需要计算![img](Piuture_5%20%E5%B7%AE%E5%88%86%E9%9A%90%E7%A7%81/3c38f78e14ad47fc8deb19e2d6269000.png)，那么为了掩盖具体数值，实际输出值应为![img](Piuture_5%20%E5%B7%AE%E5%88%86%E9%9A%90%E7%A7%81/f5f005bff95e4b128b86ca8d24d24734.png) ，相应地，另一个数据集输出的是![img](Piuture_5%20%E5%B7%AE%E5%88%86%E9%9A%90%E7%A7%81/8dbc6c7d78624e5783f776956020db4d.png)。这使得观察者分不清最终的输出是由哪个数据集产生的。

==放宽一点的差分隐私==的定义为：

![img](Piuture_5%20%E5%B7%AE%E5%88%86%E9%9A%90%E7%A7%81/c1cb2fcd21504f088d1fedc2c3e8d6a2.png)

其中 *δ*是一个比较小的常数。要获取这种差分隐私，我们可以使用高斯噪音（Gaussian noise）。

对输入或输出==加噪音会使得最终的输出结果不准确==。而且由于噪音是为了掩盖一条数据，所以很多情况下数据的多少并不影响加的噪音的量。那么在数据量很大的情况下，噪音的影响很小，这时候就可以放心大胆地加噪音了，但数据量很小的情况下，噪音的影响就显得比较大，会使得最终结果偏离准确值较远而变得不可用。


目前实际场景中主要采用的是带有松弛机制的近似差分隐私（ Approximate Differential Privacy）。

==差分隐私分类==：
**客户端**侧采用的差分隐私机制一般被称为本地化(Local)差分隐私
通过**可信中间节点进行扰动**可以被称为分布式( Distributed)差分隐私
由**服务器**完成的扰动被称为中心化(Centralized)差分隐私
而融合了上述两种或以上的差分隐私方法则被称为**混合**( Hybrid )差分隐私
（1）本地化差分隐私
本地化差分隐私意味着对数据的训练以及**对隐私的保护过程全部在客户端**就可以实现。直觉来看，这种差分隐私机制显然优于其他方案，因为用户可以全权掌握数据的使用与发布，也无需借助中心服务器，最有潜力实现完全意义上的去中心化联邦学习。

谷歌公司的Abadi等于2016年在传统机器学习中实现了差分隐私，并在当时就提出了在手机、平板电脑等小型设备上训练模型的设想，认为该差分隐私机制凭借轻量化的特点，更加适用于本地化、边缘化场景。

但是，本地化差分隐私本身及其在联邦学习的应用中仍然存在着不少**问题**。首先是它**所需求的样本量极其庞大**，例如前文所述的Snap公司将本地化差分隐私应用到垃圾邮件分类器的训练中，最终收集了用户数亿份样本才达到较高的准确度。谷歌、苹果、微软公司在用户设备上大量部署了本地化差分隐私，用来收集数据并进行模型训练，相较无噪模型的训练需要更多的数据量，往往多达2个数量级。其次，在**高维**数据下，本地化差分隐私要取得可用性、隐私性的平衡将会更加困难。

另外，在去中心化的联邦学习场景中，由于没有**中心服务器的协调**，参与者无法得知来自其他参与者的样本信息，因此很难决定自己所添加随机噪声的大小，噪声的分布不均将会严重降低模型性能。

（2）中心化差分隐私
差分隐私方法最初被提出时大多采用中心化的形式，通过一个可信的第三方数据收集者汇总数据，并对数据集进行扰动从而实现差分隐私。B2C架构下的联邦学习同样可以在中心服务器上实现这种扰动。**==在服务器端收集用户更新后的梯度，通过逐个加噪的方式来隐藏各个节点的贡献==**；并证明了中心化加噪方案可以实现用户级别的差分隐私而不仅仅是本地化方案的数据点级别，这意味着它不会暴露出任何一个曾参与过训练的用户；最后通过实验证实了这种方法的模型训练效果要优于本地化差分隐私。

中心化差分隐私在实际应用中同样存在缺陷，因为它受限于一个可信的中心化服务器，但是很多场景下**服务器并不可信**。因此，可以采用**分布式差分隐私**来作为本地化与中心化的折中，或采用混合差分隐私回避这两者的部分缺陷。

（3）分布式差分隐私
分布式差分隐私指的是**==在若干个可信中间节点上先对部分用户发送的数据进行聚合并实施隐私保护，然后传输加密或扰动后的数据到服务器端，确保服务器端只能得到聚合结果而无法得到数据==**。

该方案需要客户端首先完成计算并进行简单的扰动（例如较高隐私预算的本地化差分隐私）或加密，将结果发送至一个可信任的中间节点，

然后借助可信执行环境（TEE）、安全多方计算、安全聚合（Secure Aggregation）或安全混洗（Secure Shuffling）等方法，在中间节点实现进一步的隐私保护，

最终将结果发送至服务器端。

Bittau等于2017年提出了一种安全混洗框架Encode- Shuffle-Analyze（ESA），通过在客户端与服务器端额外增加一次匿名化混洗的步骤，允许用户在本地只添加少量噪声就实现较高级别的隐私保护。此后，Erlingsson等、Cheu等均对此框架进行了改进，并考虑了与联邦学习的结合。类似的分布式差分隐私解决方案同样都兼具了本地化与中心化差分隐私的优势，既不需要信任等级极高的服务器，也不需要在本地添加过多噪声。但相对的，分布式差分隐私普遍需要极高的**通信成本**。

本地化、中心化与分布式差分隐私的区别与联系如表所示：

![img](Piuture_5%20%E5%B7%AE%E5%88%86%E9%9A%90%E7%A7%81/340f912a4ab14b13b5e0cbe27d1240a0.png)

（4）混合差分隐私
混合差分隐私方案由Avent等提出，它通过用户对服务器信任关系的不同对用户进行分类。举例而言，最不信任服务器的用户可以使用最低隐私预算的本地化差分隐私，而最信任服务器的用户甚至可以直接发送原始参数；服务器也将根据用户的信任关系对数据进行不同程度的处理。该方案的问题是同样需要一定的通信成本，并且还需要付出额外的预处理成本以划分信任关系

4.3.1　==拉普拉斯机制==

![image-20240427184050218](Piuture_5%20%E5%B7%AE%E5%88%86%E9%9A%90%E7%A7%81/image-20240427184050218.png)

==联邦学习实验的差分隐私（服务器）==

![image-20240427184318017](Piuture_5%20%E5%B7%AE%E5%88%86%E9%9A%90%E7%A7%81/image-20240427184318017.png)

![图片](Piuture_5%20%E5%B7%AE%E5%88%86%E9%9A%90%E7%A7%81/640)

![image-20240427185111588](Piuture_5%20%E5%B7%AE%E5%88%86%E9%9A%90%E7%A7%81/image-20240427185111588.png)

![image-20240427185205264](Piuture_5%20%E5%B7%AE%E5%88%86%E9%9A%90%E7%A7%81/image-20240427185205264.png)

==联邦学习实验的差分隐私（参与方）==

防范一些非善意的参与方或服务器试图破坏隐私安全的行为。因此，**参与方在上传其梯度更新时，也将对梯度进行一些诸如梯度裁剪的处理**，以保护其隐私安全。

<img src="Piuture_5%20%E5%B7%AE%E5%88%86%E9%9A%90%E7%A7%81/image-20240427185610300.png" alt="image-20240427185610300" style="zoom:80%;" />

**==实验分析==**

本节分别对比了联邦聚合算法FedAvg和FedSGD在 加 入 差 分 隐 私 前 后 的 性 能。

将 联 邦 平 均 算 法FedAvg 的参数设置为：全局模型迭代轮次（epochs）E=100，参与方总数 N=10，每轮选择参与训练的参与方数量 n=3，参与训练的用户方本地训练轮次e=3。

联邦梯度下降算法 FedSGD 参数设置为：全局模型迭代轮次 E=100，参与方总人数 N=10，每轮选择参与训练的参与用户数量 n=10，参与训练的用户方本地训练轮次 e=1。

基于拉普拉斯噪声的差分隐私算法的参数设置为：拉普拉斯噪声的标准差=0.001与方上传的梯度的裁剪阈值 c=1。

==**收敛能力分析**==：

由图 3 可知，图中所示训练方法的模型准确率都随训练轮次的增加而逐渐增加，最后都稳定在 75% 左右，且两种聚合算法FedAvg 和 FedSGD 的收敛速度和准确率无明显区别。同时发现，添加了差分隐私的两种联邦平均算法与不添加差分隐私的原始算法的收敛速度与准确率也无明显差距。这说明，差分隐私一般不会显著影响模型的收敛性能

==**拉普拉斯噪声标准差与准确率分析：**==

随着添加的拉普拉斯噪声标准差逐渐增大，即随着隐私预算的减小，训练模型的准确率有明显的降低。若要使联邦学习训练模型达到同样的准确率和性能大小，需要训练更多的轮次才可达到这样的效果。结合图 3 可以发现，虽然差分隐私措施对于模型的收敛性可能无显著影响，但收敛后的最优效果可能随着隐私预算的减小而出现显著损失。

![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/iclynibMMTgBxDx1jVgRvx7Arf24icrgx5tIAkp1qMbD19nlAMryvnp5o2Qoko9LHeicjmbX6GI3xTb3ibBL073Ccyg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

**==裁剪阈值与准确率分析：==**
不同参与方上传的梯度裁剪阈值下的曲线几乎重合，即参与方上传的梯度裁剪的阈值的不同对模型的性能无显著影响，即不会显著影响模型训练的准确率和模型收敛性。

# 2 Ref2 一种度量联邦学习中梯度泄露程度的方法

https://mp.weixin.qq.com/s/TXpFxFru2wsMWD1HG3v2vA

https://github.com/secretflow

私有信息仍然可以通过共享梯度信息泄露。

**为了进一步保护用户的隐私，可以通过梯度信息扰动的方法防止隐私泄露，例如在与服务器共享之前使用附加噪声或梯度压缩。**

在某些防御方法的保护下，借助生成性梯度泄漏（GGL），私有训练数据仍然可以泄漏。

==**在这项工作中，我们验证了在某些防御方法的保护下，借助生成性梯度泄漏（GGL），私有训练数据仍然可以泄漏**==

与仅依赖梯度信息重建数据的现有方法不同，我们的方法利用从公共图像数据集学习的生成性对抗网络（GAN）的潜在空间作为**先验**，以补偿梯度退化期间的信息损失。

为了解决梯度算子和GAN模型引起的非线性问题，我们也探索了各种**无梯度优化方法**（例如，进化策略和贝叶斯优化），并通过实验证明了它们在从梯度重建高质量图像方面的优势，而不是基于梯度的优化器。

我们希望所提出的方法可以作为一种工具，用于根据经验**==测量隐私泄漏量==**，以便于设计更稳健的防御机制。



**对梯度进行保护的方法包括：**

- 差分隐私(differential privacy）
- 梯度裁剪(gradient clipping)
- 梯度稀疏化(gradient sparsification)
- 表征扰动(representation perturbation)

**梯度泄露导致的问题一般包括：**

- 成员推断(membership inference）
- 特性推断(attribute inference)
- 原始数据重构(recover raw data)

**难度最高的问题就是原始数据重构，本文主要解决的问题是从扰动的梯度中重构原始数据。**

![图片](Piuture_5%20%E5%B7%AE%E5%88%86%E9%9A%90%E7%A7%81/640)

一般的重构方法可能很难重构从扰动的梯度中恢复出原始数据，而利用本文提出的GGL可以较好的重构出原始数据。

# 3 Ref3 基于加噪的隐私保护算法

https://mp.weixin.qq.com/s/elYL1SewhJ0obo9AwfIf6g

可以看到，当ϵ和δ越小,可以看到，当ϵ和δ越小,

越接近，也就是说 K(x)在D和D'上面的作用结果不可区分，别人不能根据 K(D)和K(D')的输出去推断出D和D' 的不同，从而保护了数据集的差异化信息（隐私信息）。

需要设计 K(x)来对模型训练过程中的梯度进行保护。目前的保护方式有很多种（加噪，随机选择，还有啥？），这篇文章要讲的是基于加噪的差分隐私训练，这种方式又可以分为**全局差分隐私训练**和**局部差分隐私训练**。

![图片](https://mmbiz.qpic.cn/sz_mmbiz_jpg/xfBicxTBmNWVRHTND0J7TQxqfJ9FZFMakFtKUgHL8WN5f3ybwSdhTribPzF5fFFIzwoIfI2whIrObgZaLNLcVUrA/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



# 4 Ref4 虚拟同构化学习：抵御联邦学习中的数据异构性

https://mp.weixin.qq.com/s/0VxC9yegDvNQdbO7y7YrJg

可以看到，当ϵ和δ越小,![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/xfBicxTBmNWVRHTND0J7TQxqfJ9FZFMakyN2pKKpUW6DgAOpdS8UhVcU7Vaof0YXukwPlOItUGUjn9TD2pxZfjg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

越接近，也就是说 K(x)在D和D'上面的作用结果不可区分，别人不能根据 K(D)和K(D')的输出去推断出D和D' 的不同，从而保护了数据集的差异化信息（隐私信息）。